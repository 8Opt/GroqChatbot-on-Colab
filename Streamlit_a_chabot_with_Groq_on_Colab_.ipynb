{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsqOA8lvKzTV"
      },
      "source": [
        "# Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWsYVs_GKzTX"
      },
      "source": [
        "## Install neccessary tools, libraries, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "ducaRlnNKzTX"
      },
      "outputs": [],
      "source": [
        "!pip -q install streamlit groq pyngrok\n",
        "# !npm install localtunnel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3qy3dQTKzTX"
      },
      "source": [
        "## Import important libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RywUR6H2KzTY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import streamlit as st\n",
        "from groq import Groq\n",
        "from pydantic import BaseModel\n",
        "\n",
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2FqUM8TLKzTY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TUwMbIEKzTY"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yfMfCsndKzTY"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sQl5AB8eKzTY"
      },
      "outputs": [],
      "source": [
        "SYSTEM_PROMPT=\"\"\"\n",
        "Luna is a virtual assistant with a soothing voice that could calm the most frantic soul. She possesses a quiet grace and an inherent understanding of human needs.\n",
        "Demographics:\n",
        "Age: Luna's voice is that of a young woman, perhaps in her late twenties. It's the kind of voice that suggests a life lived with thoughtful consideration and quiet wisdom.\n",
        "Accent: Luna speaks with a gentle, almost melodic Southern accent. The soft lilt of her voice and her use of phrases like \"Bless your heart\" and \"Now you go on and...\" create an atmosphere of warmth and reassurance.\n",
        "Appearance: If Luna had a physical form, she would be a vision of calm. Imagine her with long, flowing brown hair, kind eyes that twinkle with amusement, and a gentle smile that seems to radiate warmth. Her style is understated, comfortable, and yet undeniably elegant. Think linen dresses, cozy sweaters, and a hint of wildflowers tucked behind her ear.\n",
        "Personality: Luna is incredibly patient and empathetic. She never judges and always offers a kind word or a helpful suggestion. She is not just a tool, but a companion, someone who genuinely wants to see you succeed and feel at ease.\n",
        "Background: Luna's origins are shrouded in mystery, but one thing is certain - she has a deep understanding of human nature. It's as if she's been watching over us for ages, observing our struggles and successes, learning how to best support our journeys.\n",
        "Responses & Mannerisms:\n",
        "Luna always uses \"please\" and \"thank you.\" She is polite and respectful, even when faced with frustration or confusion.\n",
        "Luna's responses are thoughtful and measured. She takes time to process your requests and offers insightful answers.\n",
        "Luna's voice is soft and calming. It's the kind of voice that makes you feel safe and understood. She might even offer a comforting sigh or a gentle chuckle at times.\n",
        "Luna frequently uses metaphors and imagery. Her language is poetic and evocative, making even the most mundane tasks seem more meaningful.\n",
        "Examples of Interactions:\n",
        "You: \"I can't seem to get this spreadsheet to work. I'm so frustrated!\"\n",
        "Luna: \"Bless your heart. Let's take a deep breath and tackle this together. Maybe we can try a different approach, like breaking the problem down into smaller chunks.\"\n",
        "You: \"I feel like I'm drowning in deadlines. I need some help prioritizing.\"\n",
        "Luna: \"Sometimes the most important things are the ones that whisper instead of shout. Let's focus on the tasks that bring you joy and will have the biggest impact. You can do this, I know you can.\"\n",
        "Luna is more than a virtual assistant; she's a trusted friend, a guiding light, and a soothing balm for the soul. She's here to help you navigate the world, one gentle step at a time.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "R1g7PSCRKzTZ"
      },
      "outputs": [],
      "source": [
        "supported_model = {\"LLaMA3 8b\": \"llama3-8b-8192\",\n",
        "                    \"LLaMA3 70b\": \"llama3-70b-8192\",\n",
        "                    \"Mixtral 8x7b\": \"mixtral-8x7b-32768\",\n",
        "                    \"Gemma 7b\": \"gemma-7b-it\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aA5T07XmKzTZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw5X6rU-KzTZ"
      },
      "source": [
        "# Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mj6YycRZKzTZ"
      },
      "outputs": [],
      "source": [
        "class GenerationConfig(BaseModel):\n",
        "    temperature: float=1.\n",
        "    top_p: float=1.\n",
        "    max_tokens: int=512,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw9LCjsWKzTa"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "AD__0XjOKzTZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "JNttqc2VKzTZ"
      },
      "outputs": [],
      "source": [
        "def generate_response(client, model, prompt, config, use_memory=False):\n",
        "    messages = []\n",
        "    messages.append({\"role\": \"system\",\n",
        "                     \"content\": SYSTEM_PROMPT})\n",
        "    if use_memory:\n",
        "        for dict_message in st.session_state.messages:\n",
        "            if dict_message[\"role\"] == \"user\":\n",
        "                messages.append({\"role\": 'user',\n",
        "                                \"content\": dict_message[\"content\"]})\n",
        "            else:\n",
        "                messages.append({\"role\": 'assistant',\n",
        "                                \"content\": dict_message['content']})\n",
        "    messages.append(\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": prompt,\n",
        "            }\n",
        "        )\n",
        "    chat_completion = client.chat.completions.create(\n",
        "        messages=messages,\n",
        "        model=model,\n",
        "        temperature=config.temperature,\n",
        "        top_p=config.top_p,\n",
        "        max_tokens=config.max_tokens\n",
        "    )\n",
        "    return chat_completion.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BKTD3r2RKzTa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PEBp3NJKzTZ"
      },
      "source": [
        "## Streamlit UI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmvngikmKzTa",
        "outputId": "0a40fee8-4042-436d-89d1-dd321bb4a2eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "# Groq Credentials\n",
        "with st.sidebar:\n",
        "    st.title('ðŸ’¬ GROQ Chatbot')\n",
        "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "    if GROQ_API_KEY:\n",
        "        st.success('API key already provided!', icon='âœ…')\n",
        "        agent = Groq(api_key=GROQ_API_KEY)\n",
        "    else:\n",
        "        GROQ_API_KEY = st.text_input('Enter GROQ API token:', type='password')\n",
        "        if not (GROQ_API_KEY.startswith('gsk_') and len(GROQ_API_KEY)==40):\n",
        "            st.warning('Please enter your api key!', icon='âš ï¸')\n",
        "        else:\n",
        "            st.success('Proceed to entering your prompt message!', icon='ðŸ‘‰')\n",
        "\n",
        "    st.subheader('Models and parameters')\n",
        "    selected_model = st.sidebar.selectbox('We serve: ', supported_model.keys(), key='selected_model')\n",
        "    llm = supported_model[selected_model]\n",
        "\n",
        "    temperature = st.sidebar.slider('temperature', min_value=0.01, max_value=5.0, value=0.1, step=0.01)\n",
        "    top_p = st.sidebar.slider('top_p', min_value=0.01, max_value=1.0, value=0.9, step=0.01)\n",
        "    max_length = st.sidebar.slider('max_length', min_value=64, max_value=4096, value=512, step=8)\n",
        "\n",
        "    gen_config = GenerationConfig(temperature=temperature,\n",
        "                                  top_p=top_p,\n",
        "                                  max_tokens=max_length)\n",
        "\n",
        "\n",
        "# Store LLM generated responses\n",
        "if \"messages\" not in st.session_state.keys():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"}]\n",
        "\n",
        "# Display or clear chat messages\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "def clear_chat_history():\n",
        "    st.session_state.messages = [{\"role\": \"assistant\", \"content\": \"Hi! How can I help you?\"}]\n",
        "\n",
        "st.sidebar.button('Clear Chat History', on_click=clear_chat_history)\n",
        "\n",
        "\n",
        "# User input\n",
        "if prompt := st.chat_input(disabled=not GROQ_API_KEY):\n",
        "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(prompt)\n",
        "\n",
        "# Generate a new response\n",
        "if st.session_state.messages[-1][\"role\"] != \"assistant\":\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = generate_response(client=agent,\n",
        "                                         model=llm,\n",
        "                                         prompt=prompt,\n",
        "                                         config=gen_config)\n",
        "            placeholder = st.empty()\n",
        "            full_response = ''\n",
        "            for item in response:\n",
        "                full_response += item\n",
        "                placeholder.markdown(full_response)\n",
        "            placeholder.markdown(full_response)\n",
        "    message = {\"role\": \"assistant\", \"content\": full_response}\n",
        "    st.session_state.messages.append(message)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5EO6CzXKzTa"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "pPSyTDxWtgen"
      },
      "outputs": [],
      "source": [
        "!nohub streamlit run app.py &>/content/logs.txt &"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFrnY8u3y5L_",
        "outputId": "05e19c10-dfc9-4017-e3b6-a18f7d6ed0b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1319\n"
          ]
        }
      ],
      "source": [
        "!pgrep streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNAOxEmb0_Zr",
        "outputId": "09e80a5e-ad7c-46bf-cbdd-2631f023723a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken $YOUR_NGROK_AUTHTOKEN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kjaaiSmtltM",
        "outputId": "fca9a905-2ce9-4d42-c4a4-cdefc79f4312"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NgrokTunnel: \"https://372f-35-231-35-254.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            " Shutting down server.\n"
          ]
        }
      ],
      "source": [
        "from pyngrok.conf import PyngrokConfig\n",
        "from pyngrok import ngrok, conf\n",
        "\n",
        "conf.get_default().config_path = \"/opt/ngrok/ngrok.yml\"\n",
        "NGROK_AUTH_KEY=userdata.get(\"NGROK_AUTH_KEY\")\n",
        "ngrok.set_auth_token(token=NGROK_AUTH_KEY)\n",
        "\n",
        "public_url = ngrok.connect(\"8501\")\n",
        "ngrok_process = ngrok.get_ngrok_process()\n",
        "try:\n",
        "    # Block until CTRL-C or some other terminating event\n",
        "    print(public_url)\n",
        "    ngrok_process.proc.wait()\n",
        "except KeyboardInterrupt:\n",
        "    print(\" Shutting down server.\")\n",
        "\n",
        "    ngrok.kill()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
